{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a410373-875d-4269-aaa5-1d9be6ec79eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: train_tharu.csv\n",
      "\n",
      "Done!\n",
      "Total unique cleaned sentences: 8,100\n",
      "Saved to: tharu_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "csv_files = [\"train_tharu.csv\"]\n",
    "output_file = \"tharu_corpus.txt\"\n",
    "\n",
    "'''Remove:\n",
    "English letters\n",
    "English digits\n",
    "Devanagari digits\n",
    "Special characters\n",
    "'''\n",
    "CLEAN_RE = re.compile(\n",
    "    r\"[A-Za-z0-9\\u0966-\\u096F,.\\-!?;:\\\"'()\\[\\]{}<>@#$%^&*_+=/\\\\|~`]\"\n",
    ")\n",
    "\n",
    "unique_sentences = set()\n",
    "final_sentences = []\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Reading: {csv_path}\")\n",
    "\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)  # skip header\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) > 1:\n",
    "                text = row[1].strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                # Split by danda\n",
    "                for sentence in re.split(r\"[।॥]\", text):\n",
    "                    s = sentence.strip()\n",
    "                    s = s.rstrip(\"|\")                  #remove trailing |\n",
    "                    s = CLEAN_RE.sub(\"\", s)            #remove unwanted characters\n",
    "                    s = re.sub(r\"\\s+\", \" \", s).strip() #remove extra spaces\n",
    "\n",
    "                    if len(s) < 3:\n",
    "                        continue\n",
    "\n",
    "                    # Deduplication\n",
    "                    if s not in unique_sentences:\n",
    "                        unique_sentences.add(s)\n",
    "                        final_sentences.append(s)\n",
    "\n",
    "# save to cleaned corpus\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for s in final_sentences:\n",
    "        out.write(s + \"\\n\")\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "print(f\"Total unique cleaned sentences: {len(final_sentences):,}\")\n",
    "print(f\"Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f985a-ff1d-4477-857c-5c62e3dcde92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c756f3d4-a39e-45f3-835d-81679d56e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Tharu (THL / 3062) – 27 books ≈ 260 chapters\n",
      "\n",
      "→ MAT (28 chapters)\n",
      "   1  done  (1/260)\n",
      "   2  done  (2/260)\n",
      "   3  done  (3/260)\n",
      "   4  done  (4/260)\n",
      "   5  done  (5/260)\n",
      "   6  done  (6/260)\n",
      "   7  done  (7/260)\n",
      "   8  done  (8/260)\n",
      "   9  done  (9/260)\n",
      "  10  done  (10/260)\n",
      "  11  done  (11/260)\n",
      "  12  done  (12/260)\n",
      "  13  done  (13/260)\n",
      "  14  done  (14/260)\n",
      "  15  done  (15/260)\n",
      "  16  done  (16/260)\n",
      "  17  done  (17/260)\n",
      "  18  done  (18/260)\n",
      "  19  done  (19/260)\n",
      "  20  done  (20/260)\n",
      "  21  done  (21/260)\n",
      "  22  done  (22/260)\n",
      "  23  done  (23/260)\n",
      "  24  done  (24/260)\n",
      "  25  done  (25/260)\n",
      "  26  done  (26/260)\n",
      "  27  done  (27/260)\n",
      "  28  done  (28/260)\n",
      "→ MRK (16 chapters)\n",
      "   1  done  (29/260)\n",
      "   2  done  (30/260)\n",
      "   3  done  (31/260)\n",
      "   4  done  (32/260)\n",
      "   5  done  (33/260)\n",
      "   6  done  (34/260)\n",
      "   7  done  (35/260)\n",
      "   8  done  (36/260)\n",
      "   9  done  (37/260)\n",
      "  10  done  (38/260)\n",
      "  11  done  (39/260)\n",
      "  12  done  (40/260)\n",
      "  13  done  (41/260)\n",
      "  14  done  (42/260)\n",
      "  15  done  (43/260)\n",
      "  16  done  (44/260)\n",
      "→ LUK (24 chapters)\n",
      "   1  done  (45/260)\n",
      "   2  done  (46/260)\n",
      "   3  done  (47/260)\n",
      "   4  done  (48/260)\n",
      "   5  done  (49/260)\n",
      "   6  done  (50/260)\n",
      "   7  done  (51/260)\n",
      "   8  done  (52/260)\n",
      "   9  done  (53/260)\n",
      "  10  done  (54/260)\n",
      "  11  done  (55/260)\n",
      "  12  done  (56/260)\n",
      "  13  done  (57/260)\n",
      "  14  done  (58/260)\n",
      "  15  done  (59/260)\n",
      "  16  done  (60/260)\n",
      "  17  done  (61/260)\n",
      "  18  done  (62/260)\n",
      "  19  done  (63/260)\n",
      "  20  done  (64/260)\n",
      "  21  done  (65/260)\n",
      "  22  done  (66/260)\n",
      "  23  done  (67/260)\n",
      "  24  done  (68/260)\n",
      "→ JHN (21 chapters)\n",
      "   1  done  (69/260)\n",
      "   2  done  (70/260)\n",
      "   3  done  (71/260)\n",
      "   4  done  (72/260)\n",
      "   5  done  (73/260)\n",
      "   6  done  (74/260)\n",
      "   7  done  (75/260)\n",
      "   8  done  (76/260)\n",
      "   9  done  (77/260)\n",
      "  10  done  (78/260)\n",
      "  11  done  (79/260)\n",
      "  12  done  (80/260)\n",
      "  13  done  (81/260)\n",
      "  14  done  (82/260)\n",
      "  15  done  (83/260)\n",
      "  16  done  (84/260)\n",
      "  17  done  (85/260)\n",
      "  18  done  (86/260)\n",
      "  19  done  (87/260)\n",
      "  20  done  (88/260)\n",
      "  21  done  (89/260)\n",
      "→ ACT (28 chapters)\n",
      "   1  done  (90/260)\n",
      "   2  done  (91/260)\n",
      "   3  done  (92/260)\n",
      "   4  done  (93/260)\n",
      "   5  done  (94/260)\n",
      "   6  done  (95/260)\n",
      "   7  done  (96/260)\n",
      "   8  done  (97/260)\n",
      "   9  done  (98/260)\n",
      "  10  done  (99/260)\n",
      "  11  done  (100/260)\n",
      "  12  done  (101/260)\n",
      "  13  done  (102/260)\n",
      "  14  done  (103/260)\n",
      "  15  done  (104/260)\n",
      "  16  done  (105/260)\n",
      "  17  done  (106/260)\n",
      "  18  done  (107/260)\n",
      "  19  done  (108/260)\n",
      "  20  done  (109/260)\n",
      "  21  done  (110/260)\n",
      "  22  done  (111/260)\n",
      "  23  done  (112/260)\n",
      "  24  done  (113/260)\n",
      "  25  done  (114/260)\n",
      "  26  done  (115/260)\n",
      "  27  done  (116/260)\n",
      "  28  done  (117/260)\n",
      "→ ROM (16 chapters)\n",
      "   1  done  (118/260)\n",
      "   2  done  (119/260)\n",
      "   3  done  (120/260)\n",
      "   4  done  (121/260)\n",
      "   5  done  (122/260)\n",
      "   6  done  (123/260)\n",
      "   7  done  (124/260)\n",
      "   8  done  (125/260)\n",
      "   9  done  (126/260)\n",
      "  10  done  (127/260)\n",
      "  11  done  (128/260)\n",
      "  12  done  (129/260)\n",
      "  13  done  (130/260)\n",
      "  14  done  (131/260)\n",
      "  15  done  (132/260)\n",
      "  16  done  (133/260)\n",
      "→ 1CO (16 chapters)\n",
      "   1  done  (134/260)\n",
      "   2  done  (135/260)\n",
      "   3  done  (136/260)\n",
      "   4  done  (137/260)\n",
      "   5  done  (138/260)\n",
      "   6  done  (139/260)\n",
      "   7  done  (140/260)\n",
      "   8  done  (141/260)\n",
      "   9  done  (142/260)\n",
      "  10  done  (143/260)\n",
      "  11  done  (144/260)\n",
      "  12  done  (145/260)\n",
      "  13  done  (146/260)\n",
      "  14  done  (147/260)\n",
      "  15  done  (148/260)\n",
      "  16  done  (149/260)\n",
      "→ 2CO (13 chapters)\n",
      "   1  done  (150/260)\n",
      "   2  done  (151/260)\n",
      "   3  done  (152/260)\n",
      "   4  done  (153/260)\n",
      "   5  done  (154/260)\n",
      "   6  done  (155/260)\n",
      "   7  done  (156/260)\n",
      "   8  done  (157/260)\n",
      "   9  done  (158/260)\n",
      "  10  done  (159/260)\n",
      "  11  done  (160/260)\n",
      "  12  done  (161/260)\n",
      "  13  done  (162/260)\n",
      "→ GAL (6 chapters)\n",
      "   1  done  (163/260)\n",
      "   2  done  (164/260)\n",
      "   3  done  (165/260)\n",
      "   4  done  (166/260)\n",
      "   5  done  (167/260)\n",
      "   6  done  (168/260)\n",
      "→ EPH (6 chapters)\n",
      "   1  done  (169/260)\n",
      "   2  done  (170/260)\n",
      "   3  done  (171/260)\n",
      "   4  done  (172/260)\n",
      "   5  done  (173/260)\n",
      "   6  done  (174/260)\n",
      "→ PHP (4 chapters)\n",
      "   1  done  (175/260)\n",
      "   2  done  (176/260)\n",
      "   3  done  (177/260)\n",
      "   4  done  (178/260)\n",
      "→ COL (4 chapters)\n",
      "   1  done  (179/260)\n",
      "   2  done  (180/260)\n",
      "   3  done  (181/260)\n",
      "   4  done  (182/260)\n",
      "→ 1TH (5 chapters)\n",
      "   1  done  (183/260)\n",
      "   2  done  (184/260)\n",
      "   3  done  (185/260)\n",
      "   4  done  (186/260)\n",
      "   5  done  (187/260)\n",
      "→ 2TH (3 chapters)\n",
      "   1  done  (188/260)\n",
      "   2  done  (189/260)\n",
      "   3  done  (190/260)\n",
      "→ 1TI (6 chapters)\n",
      "   1  done  (191/260)\n",
      "   2  done  (192/260)\n",
      "   3  done  (193/260)\n",
      "   4  done  (194/260)\n",
      "   5  done  (195/260)\n",
      "   6  done  (196/260)\n",
      "→ 2TI (4 chapters)\n",
      "   1  done  (197/260)\n",
      "   2  done  (198/260)\n",
      "   3  done  (199/260)\n",
      "   4  done  (200/260)\n",
      "→ TIT (3 chapters)\n",
      "   1  done  (201/260)\n",
      "   2  done  (202/260)\n",
      "   3  done  (203/260)\n",
      "→ PHM (1 chapters)\n",
      "   1  done  (204/260)\n",
      "→ HEB (13 chapters)\n",
      "   1  done  (205/260)\n",
      "   2  done  (206/260)\n",
      "   3  done  (207/260)\n",
      "   4  done  (208/260)\n",
      "   5  done  (209/260)\n",
      "   6  done  (210/260)\n",
      "   7  done  (211/260)\n",
      "   8  done  (212/260)\n",
      "   9  done  (213/260)\n",
      "  10  done  (214/260)\n",
      "  11  done  (215/260)\n",
      "  12  done  (216/260)\n",
      "  13  done  (217/260)\n",
      "→ JAS (5 chapters)\n",
      "   1  done  (218/260)\n",
      "   2  done  (219/260)\n",
      "   3  done  (220/260)\n",
      "   4  done  (221/260)\n",
      "   5  done  (222/260)\n",
      "→ 1PE (5 chapters)\n",
      "   1  done  (223/260)\n",
      "   2  done  (224/260)\n",
      "   3  done  (225/260)\n",
      "   4  done  (226/260)\n",
      "   5  done  (227/260)\n",
      "→ 2PE (3 chapters)\n",
      "   1  done  (228/260)\n",
      "   2  done  (229/260)\n",
      "   3  done  (230/260)\n",
      "→ 1JN (5 chapters)\n",
      "   1  done  (231/260)\n",
      "   2  done  (232/260)\n",
      "   3  done  (233/260)\n",
      "   4  done  (234/260)\n",
      "   5  done  (235/260)\n",
      "→ 2JN (1 chapters)\n",
      "   1  done  (236/260)\n",
      "→ 3JN (1 chapters)\n",
      "   1  done  (237/260)\n",
      "→ JUD (1 chapters)\n",
      "   1  done  (238/260)\n",
      "→ REV (22 chapters)\n",
      "   1  done  (239/260)\n",
      "   2  done  (240/260)\n",
      "   3  done  (241/260)\n",
      "   4  done  (242/260)\n",
      "   5  done  (243/260)\n",
      "   6  done  (244/260)\n",
      "   7  done  (245/260)\n",
      "   8  done  (246/260)\n",
      "   9  done  (247/260)\n",
      "  10  done  (248/260)\n",
      "  11  done  (249/260)\n",
      "  12  done  (250/260)\n",
      "  13  done  (251/260)\n",
      "  14  done  (252/260)\n",
      "  15  done  (253/260)\n",
      "  16  done  (254/260)\n",
      "  17  done  (255/260)\n",
      "  18  done  (256/260)\n",
      "  19  done  (257/260)\n",
      "  20  done  (258/260)\n",
      "  21  done  (259/260)\n",
      "  22  done  (260/260)\n",
      "\n",
      "Done. Saved to: /Users/Dell/Desktop/finalproj/nepal-lang/tharu_bible.txt\n",
      "Total lines: 9,545\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "VERSION_ID = 3062           #Tharu, Dāngaurā Tharu\n",
    "OUTPUT_FILE = Path(\"tharu_bible.txt\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://www.bible.com/bible/{vid}/{book}.{chapter}.THL\"\n",
    "\n",
    "#New Testament books (USFM code + number of chapters)\n",
    "BOOKS = [\n",
    "    (\"MAT\", 28), (\"MRK\", 16), (\"LUK\", 24), (\"JHN\", 21),\n",
    "    (\"ACT\", 28), (\"ROM\", 16), (\"1CO\", 16), (\"2CO\", 13),\n",
    "    (\"GAL\", 6),  (\"EPH\", 6),  (\"PHP\", 4),  (\"COL\", 4),\n",
    "    (\"1TH\", 5),  (\"2TH\", 3),  (\"1TI\", 6),  (\"2TI\", 4),\n",
    "    (\"TIT\", 3),  (\"PHM\", 1),  (\"HEB\", 13), (\"JAS\", 5),\n",
    "    (\"1PE\", 5),  (\"2PE\", 3),  (\"1JN\", 5),  (\"2JN\", 1),\n",
    "    (\"3JN\", 1),  (\"JUD\", 1),  (\"REV\", 22)\n",
    "]\n",
    "\n",
    "def clean_verse_text(text: str) -> str:\n",
    "    \"\"\"Clean verse text: remove verse numbers, footnotes, collapse spaces\"\"\"\n",
    "    #Remove leading verse number\n",
    "    text = re.sub(r'^\\d+\\s*', '', text.strip())\n",
    "    #Remove footnote markers\n",
    "    text = re.sub(r'#.*?(?=\\s|$)', '', text)\n",
    "    #Collapse multiple spaces/newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    #Remove trailing punctuation\n",
    "    text = re.sub(r'\\s*[\\।।]?\\s*$', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def scrape_chapter(book_code: str, chapter: int) -> Optional[List[str]]:\n",
    "    url = BASE_URL.format(vid=VERSION_ID, book=book_code, chapter=chapter)\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=12)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\" Failed to fetch {book_code} {chapter}  ({e})\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    #Main content container\n",
    "    container = soup.find(\"div\", class_=\"ChapterContent_bible-reader__LmLUa\")\n",
    "    if not container:\n",
    "        print(f\"  → No chapter content found for {book_code} {chapter}\")\n",
    "        return None\n",
    "\n",
    "    verses = []\n",
    "\n",
    "    for verse_span in container.find_all(\"span\", class_=\"ChapterContent_verse__57FIw\"):\n",
    "        #Verse number\n",
    "        label = verse_span.find(\"span\", class_=\"ChapterContent_label__R2PLt\")\n",
    "        verse_num = label.get_text(strip=True) if label else \"?\"\n",
    "\n",
    "        #Collect text\n",
    "        text_parts = []\n",
    "        for child in verse_span.descendants:\n",
    "            if child.name == \"span\":\n",
    "                classes = child.get(\"class\", [])\n",
    "                if \"label\" in classes or \"note\" in classes or \"heading\" in classes:\n",
    "                    continue\n",
    "            if isinstance(child, str) and child.strip():\n",
    "                text_parts.append(child.strip())\n",
    "\n",
    "        verse_text = clean_verse_text(\" \".join(text_parts))\n",
    "        if verse_text:\n",
    "            verses.append(verse_text)\n",
    "\n",
    "    return verses\n",
    "\n",
    "\n",
    "def main():\n",
    "    output_lines = []\n",
    "    total_chapters = sum(chap for _, chap in BOOKS)\n",
    "\n",
    "    print(f\"Scraping Tharu (THL / 3062) – {len(BOOKS)} books ≈ {total_chapters} chapters\\n\")\n",
    "\n",
    "    processed = 0\n",
    "\n",
    "    for book_code, max_chap in BOOKS:\n",
    "        print(f\"→ {book_code} ({max_chap} chapters)\")\n",
    "        book_lines = [f\"\\n=== {book_code} ===\\n\"]\n",
    "\n",
    "        for chap in range(1, max_chap + 1):\n",
    "            processed += 1\n",
    "            verses = scrape_chapter(book_code, chap)\n",
    "\n",
    "            if verses:\n",
    "                book_lines.append(f\"--- Chapter {chap} ---\")\n",
    "                book_lines.extend(verses)\n",
    "                book_lines.append(\"\")\n",
    "            else:\n",
    "                book_lines.append(f\"(Chapter {chap} failed or not available)\")\n",
    "\n",
    "            print(f\"  {chap:2d}  done  ({processed}/{total_chapters})\")\n",
    "            time.sleep(2.4)   # polite delay (~25 req/min)\n",
    "\n",
    "        output_lines.extend(book_lines)\n",
    "\n",
    "    # Save\n",
    "    with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(line.rstrip() for line in output_lines if line.strip()) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nDone. Saved to: {OUTPUT_FILE.absolute()}\")\n",
    "    print(f\"Total lines: {len(output_lines):,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eace7db-5da6-4fb5-bbc0-1a34b7d6c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done!\n",
      "Total sentences read: 23973\n",
      "Duplicates removed: 2157\n",
      "Total unique sentences saved: 21816\n",
      "Saved cleaned corpus to: /Users/Dell/Desktop/finalproj/nepal-lang/tharu_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "files_to_merge = [\n",
    "    \"tharu_bible.txt\",\n",
    "    \"tharu_corpus.txt\"\n",
    "]\n",
    "\n",
    "output_file = Path(\"tharu_cleaned.txt\")\n",
    "\n",
    "'''Remove:\n",
    "English letters\n",
    "English digits\n",
    "Devanagari digits\n",
    "Special characters\n",
    "'''\n",
    "clean_pattern = re.compile(r\"[A-Za-z0-9\\u0966-\\u096F,.\\-!?;:\\\"'()\\[\\]{}<>@#$%^&*_+=/\\\\|~`]\")\n",
    "\n",
    "# Ordered set to remove duplicates while preserving order\n",
    "seen = OrderedDict()\n",
    "\n",
    "total_sentences_read = 0\n",
    "total_duplicates_removed = 0\n",
    "\n",
    "with output_file.open('w', encoding='utf-8') as outfile:\n",
    "    for fname in files_to_merge:\n",
    "        if not os.path.exists(fname):\n",
    "            print(f\"File not found: {fname}\")\n",
    "            continue\n",
    "\n",
    "        with open(fname, 'r', encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # Remove unwanted characters\n",
    "                line = clean_pattern.sub('', line)\n",
    "\n",
    "                # Normalize spaces\n",
    "                line = re.sub(r'\\s+', ' ', line).strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # Split into sentences by Devanagari danda\n",
    "                sentences = re.split(r'[।॥]', line)\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if not sentence:\n",
    "                        continue\n",
    "\n",
    "                    total_sentences_read += 1\n",
    "\n",
    "                    # Deduplicate\n",
    "                    if sentence not in seen:\n",
    "                        seen[sentence] = None\n",
    "                        outfile.write(sentence + \"\\n\")\n",
    "                    else:\n",
    "                        total_duplicates_removed += 1\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"Total sentences read: {total_sentences_read}\")\n",
    "print(f\"Duplicates removed: {total_duplicates_removed}\")\n",
    "print(f\"Total unique sentences saved: {len(seen)}\")\n",
    "print(f\"Saved cleaned corpus to: {output_file.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6102c-d272-40cb-b790-20041a5a7729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
