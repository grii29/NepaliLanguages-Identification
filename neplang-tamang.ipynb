{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd8cc07-15d3-43a4-8ab7-4b697958ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recent articles from homepage...\n",
      "\n",
      "Found 55 potential articles\n",
      "\n",
      "असारे बजेट : भ्रष्टाचारला खेती\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/6chclrp56lsgtty2sn1y79200000gn/T/ipykernel_7993/1473399973.py:62: UserWarning: Ignoring nested list {'class': re.compile('sharedaddy|sd-|advert|banner', re.IGNORECASE)} to avoid the possibility of infinite recursion.\n",
      "  for bad in content_div.find_all([\"script\", \"style\", \"iframe\", \"form\",\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone. Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m readable articles tamangnews_scraped1.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m             \u001b[38;5;28mprint\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     96\u001b[39m             f.write(data[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_sentences\u001b[49m:\n\u001b[32m     99\u001b[39m                 out.write(sentence + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone. Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m readable articles tamangnews_scraped1.txt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_article_links(home_url=\"https://www.tamangdajang.com/\"):\n",
    "    try:\n",
    "        r = requests.get(home_url, timeout=12)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"Homepage failed \", e)\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if re.search(r'/(whaiim|tamsor|sengrap|mhasengrap|lehmngo|bichar|rimthim|gyalthim)/\\d+', href):\n",
    "            full = urljoin(home_url, href)\n",
    "            if \"tamangdajang.com\" in full:\n",
    "                links.add(full)\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=12)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed {url} → {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Title\n",
    "    title_tag = soup.find(\"h1\", class_=\"single-heading\") or soup.find(\"h1\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No title\"\n",
    "\n",
    "    # Author & date\n",
    "    meta = soup.find(\"div\", class_=\"post-info\")\n",
    "    author = \"Unknown\"\n",
    "    date_str = \"Unknown\"\n",
    "    if meta:\n",
    "        author_a = meta.find(\"a\")\n",
    "        if author_a:\n",
    "            author = author_a.get_text(strip=True)\n",
    "        date_span = meta.find(\"span\", class_=\"rddate\")\n",
    "        if date_span:\n",
    "            date_str = date_span.get_text(strip=True)\n",
    "\n",
    "    # Main content\n",
    "    content_div = soup.find(\"div\", class_=\"news-body-content\") or \\\n",
    "                  soup.find(\"div\", class_=\"news-body\") or \\\n",
    "                  soup.find(\"div\", itemprop=\"articleBody\")\n",
    "\n",
    "    if not content_div:\n",
    "        return {\"url\": url, \"title\": title, \"author\": author, \"date\": date_str, \"content\": \"No content found\"}\n",
    "\n",
    "    # Clean junk\n",
    "    for bad in content_div.find_all([\"script\", \"style\", \"iframe\", \"form\",\n",
    "                               {\"class\": re.compile(\"sharedaddy|sd-|advert|banner\", re.I)}]):\n",
    "        bad.decompose()\n",
    "\n",
    "    # ---- Extract paragraphs and remove English letters/digits ----\n",
    "    paragraphs = [\n",
    "    re.sub(r'[A-Za-z0-9]', '', p.get_text(strip=True))  # removes English letters & digits\n",
    "    for p in content_div.find_all(\"p\")\n",
    "    if p.get_text(strip=True)\n",
    "    ]\n",
    "    return {\n",
    "    \"url\": url,\n",
    "    \"title\": title,\n",
    "    \"content\": \"\\n\\n\".join(paragraphs) if paragraphs else \"No readable text\"\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Collecting recent articles from homepage...\\n\")\n",
    "    urls = get_article_links()\n",
    "\n",
    "    if not urls:\n",
    "        print(\"No article links found on homepage.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(urls)} potential articles\\n\")\n",
    "\n",
    "    saved = 0\n",
    "    with open(\"tamangnews_scraped1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, url in enumerate(urls[:], 1): \n",
    "            data = scrape_article(url)\n",
    "            if data and len(data[\"content\"]) > 120:\n",
    "                saved += 1\n",
    "                print(data['title'])\n",
    "                f.write(data['title'] + \"\\n\")\n",
    "\n",
    "                for sentence in data[\"content\"].split(\"।\"):\n",
    "                    s = sentence.strip()\n",
    "                    if s:\n",
    "                        f.write(s + \"\\n\")\n",
    "\n",
    "    print(f\"\\nDone. Saved {saved} readable articles tamangnews_scraped1.txt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16488781-d17e-4b64-ab6f-c09b859914ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भजनगदे (Deva-01-XXE) – पूर्ण संग्रह डाउनलोड सुरु हुँदैछ...\n",
      "\n",
      "Trying to list chapters from base URL...\n",
      "No links found in index → generating expected URLs\n",
      "Found / generated 53 chapter URLs\n",
      "\n",
      "  Scraping 01 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-001.html\n",
      "  Scraping 02 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-002.html\n",
      "  Scraping 03 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-003.html\n",
      "  Scraping 04 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-004.html\n",
      "  Scraping 05 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-005.html\n",
      "  Scraping 06 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-006.html\n",
      "  Scraping 07 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-007.html\n",
      "  Scraping 08 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-008.html\n",
      "  Scraping 09 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-009.html\n",
      "  Scraping 10 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-010.html\n",
      "  Scraping 11 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-011.html\n",
      "  Scraping 12 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-012.html\n",
      "  Scraping 13 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-013.html\n",
      "  Scraping 14 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-014.html\n",
      "  Scraping 15 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-015.html\n",
      "  Scraping 16 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-016.html\n",
      "  Scraping 17 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-017.html\n",
      "  Scraping 18 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-018.html\n",
      "  Scraping 19 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-019.html\n",
      "  Scraping 20 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-020.html\n",
      "  Scraping 21 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-021.html\n",
      "  Scraping 22 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-022.html\n",
      "  Scraping 23 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-023.html\n",
      "  Scraping 24 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-024.html\n",
      "  Scraping 25 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-025.html\n",
      "  Scraping 26 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-026.html\n",
      "  Scraping 27 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-027.html\n",
      "  Scraping 28 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-028.html\n",
      "  Scraping 29 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-029.html\n",
      "  Scraping 30 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-030.html\n",
      "  Scraping 31 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-031.html\n",
      "  Scraping 32 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-032.html\n",
      "  Scraping 33 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-033.html\n",
      "  Scraping 34 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-034.html\n",
      "  Scraping 35 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-035.html\n",
      "  Scraping 36 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-036.html\n",
      "  Scraping 37 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-037.html\n",
      "  Scraping 38 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-038.html\n",
      "  Scraping 39 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-039.html\n",
      "  Scraping 40 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-040.html\n",
      "  Scraping 41 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-041.html\n",
      "  Scraping 42 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-042.html\n",
      "  Scraping 43 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-043.html\n",
      "  Scraping 44 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-044.html\n",
      "  Scraping 45 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-045.html\n",
      "  Scraping 46 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-046.html\n",
      "  Scraping 47 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-047.html\n",
      "  Scraping 48 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-048.html\n",
      "  Scraping 49 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-049.html\n",
      "  Scraping 50 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-050.html\n",
      "  Scraping 51 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-051.html\n",
      "  Scraping 52 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-052.html\n",
      "  Scraping 53 → https://media.ipsapps.org/taj/osa/hymns/C01/Deva-01-XXE-053.html\n",
      "सम्पन्न! 53 भजनहरू सफलतापूर्वक बचत गरियो\n",
      "फाइल नाम:Tamang_bible2.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://media.ipsapps.org/taj/osa/hymns/C01/\"\n",
    "CHAPTER_PATTERN = r\"Deva-01-XXE-\\d{3}\\.html\"   # matches Deva-01-XXE-001.html etc.\n",
    "START = 1\n",
    "END = 53\n",
    "DELAY = 1.2  # seconds between requests (polite to server)\n",
    "\n",
    "def get_all_chapter_urls():\n",
    "    print(\"Trying to list chapters from base URL...\")\n",
    "    try:\n",
    "        r = requests.get(BASE_URL, timeout=12)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"Cannot access directory index →\", e)\n",
    "        print(\"Falling back to direct URL pattern (assuming 001–053 exist)...\")\n",
    "        urls = []\n",
    "        for i in range(START, END + 1):\n",
    "            num = f\"{i:03d}\"\n",
    "            url = urljoin(BASE_URL, f\"Deva-01-XXE-{num}.html\")\n",
    "            urls.append(url)\n",
    "        return urls\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if re.search(CHAPTER_PATTERN, href):\n",
    "            full = urljoin(BASE_URL, href)\n",
    "            links.add(full)\n",
    "\n",
    "    if not links:\n",
    "        print(\"No links found in index → generating expected URLs\")\n",
    "        for i in range(START, END + 1):\n",
    "            num = f\"{i:03d}\"\n",
    "            url = urljoin(BASE_URL, f\"Deva-01-XXE-{num}.html\")\n",
    "            links.add(url)\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "def scrape_chapter(url, chapter_num):\n",
    "    print(f\"  Scraping {chapter_num:02d} → {url}\")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=12)\n",
    "        r.encoding = 'utf-8'\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"    → Failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Title\n",
    "    title_tag = soup.find(\"title\") or soup.find(\"div\", class_=\"mt\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else f\"भजन {chapter_num}\"\n",
    "\n",
    "    # Lyrics content: <div class=\"li\">, <div class=\"li2\">, <div class=\"s\">\n",
    "    verses = []\n",
    "    for tag in soup.find_all([\"div\"], class_=[\"li\", \"li2\", \"s\"]):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if text and text.strip(\"।। \"):  # skip empty/decorative\n",
    "            verses.append(text)\n",
    "\n",
    "    content = \"\\n\".join(verses) if verses else \"(कुनै सामग्री फेला परेन)\"\n",
    "\n",
    "    return title, content\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"भजनगदे (Deva-01-XXE) – पूर्ण संग्रह डाउनलोड सुरु हुँदैछ...\\n\")\n",
    "    \n",
    "    urls = get_all_chapter_urls()\n",
    "    print(f\"Found / generated {len(urls)} chapter URLs\\n\")\n",
    "\n",
    "    total_saved = 0\n",
    "\n",
    "    with open(\"Tamang_bible2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, url in enumerate(urls, 1):\n",
    "            chapter_num = idx  # assuming order 1 to 53\n",
    "            title, content = scrape_chapter(url, chapter_num)\n",
    "\n",
    "            if title and content and \"फेला परेन\" not in content:\n",
    "                total_saved += 1\n",
    "                f.write(f\"भजन {chapter_num:02d}   –   {title}\\n\")\n",
    "                f.write(content + \"\\n\\n\")\n",
    "            else:\n",
    "                f.write(f\"भजन {chapter_num:02d}   –   (उपलब्ध छैन वा खाली)\\n\")\n",
    "\n",
    "\n",
    "            time.sleep(DELAY)\n",
    "\n",
    "    print(f\"सम्पन्न! {total_saved} भजनहरू सफलतापूर्वक बचत गरियो\")\n",
    "    print(f\"फाइल नाम:Tamang_bible2.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2971fe4-a661-41e4-9156-c8a5b61463c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape of Eastern Tamang NT (tajNT) – 27 books, ~260 chapters\n",
      "\n",
      "→ MAT (28 chapters)\n",
      "  Chapter  1  done  (1/260)\n",
      "  Chapter  2  done  (2/260)\n",
      "  Chapter  3  done  (3/260)\n",
      "  Chapter  4  done  (4/260)\n",
      "  Chapter  5  done  (5/260)\n",
      "  Chapter  6  done  (6/260)\n",
      "  Chapter  7  done  (7/260)\n",
      "  Chapter  8  done  (8/260)\n",
      "  Chapter  9  done  (9/260)\n",
      "  Chapter 10  done  (10/260)\n",
      "  Chapter 11  done  (11/260)\n",
      "  Chapter 12  done  (12/260)\n",
      "  Chapter 13  done  (13/260)\n",
      "  Chapter 14  done  (14/260)\n",
      "  Chapter 15  done  (15/260)\n",
      "  Chapter 16  done  (16/260)\n",
      "  Chapter 17  done  (17/260)\n",
      "  Chapter 18  done  (18/260)\n",
      "  Chapter 19  done  (19/260)\n",
      "  Chapter 20  done  (20/260)\n",
      "  Chapter 21  done  (21/260)\n",
      "  Chapter 22  done  (22/260)\n",
      "  Chapter 23  done  (23/260)\n",
      "  Chapter 24  done  (24/260)\n",
      "  Chapter 25  done  (25/260)\n",
      "  Chapter 26  done  (26/260)\n",
      "  Chapter 27  done  (27/260)\n",
      "  Chapter 28  done  (28/260)\n",
      "→ MRK (16 chapters)\n",
      "  Chapter  1  done  (29/260)\n",
      "  Chapter  2  done  (30/260)\n",
      "  Chapter  3  done  (31/260)\n",
      "  Chapter  4  done  (32/260)\n",
      "  Chapter  5  done  (33/260)\n",
      "  Chapter  6  done  (34/260)\n",
      "  Chapter  7  done  (35/260)\n",
      "  Chapter  8  done  (36/260)\n",
      "  Chapter  9  done  (37/260)\n",
      "  Chapter 10  done  (38/260)\n",
      "  Chapter 11  done  (39/260)\n",
      "  Chapter 12  done  (40/260)\n",
      "  Chapter 13  done  (41/260)\n",
      "  Chapter 14  done  (42/260)\n",
      "  Chapter 15  done  (43/260)\n",
      "  Chapter 16  done  (44/260)\n",
      "→ LUK (24 chapters)\n",
      "  Chapter  1  done  (45/260)\n",
      "  Chapter  2  done  (46/260)\n",
      "  Chapter  3  done  (47/260)\n",
      "  Chapter  4  done  (48/260)\n",
      "  Chapter  5  done  (49/260)\n",
      "  Chapter  6  done  (50/260)\n",
      "  Chapter  7  done  (51/260)\n",
      "  Chapter  8  done  (52/260)\n",
      "  Chapter  9  done  (53/260)\n",
      "  Chapter 10  done  (54/260)\n",
      "  Chapter 11  done  (55/260)\n",
      "  Chapter 12  done  (56/260)\n",
      "  Chapter 13  done  (57/260)\n",
      "  Chapter 14  done  (58/260)\n",
      "  Chapter 15  done  (59/260)\n",
      "  Chapter 16  done  (60/260)\n",
      "  Chapter 17  done  (61/260)\n",
      "  Chapter 18  done  (62/260)\n",
      "  Chapter 19  done  (63/260)\n",
      "  Chapter 20  done  (64/260)\n",
      "  Chapter 21  done  (65/260)\n",
      "  Chapter 22  done  (66/260)\n",
      "  Chapter 23  done  (67/260)\n",
      "  Chapter 24  done  (68/260)\n",
      "→ JHN (21 chapters)\n",
      "  Chapter  1  done  (69/260)\n",
      "  Chapter  2  done  (70/260)\n",
      "  Chapter  3  done  (71/260)\n",
      "  Chapter  4  done  (72/260)\n",
      "  Chapter  5  done  (73/260)\n",
      "  Chapter  6  done  (74/260)\n",
      "  Chapter  7  done  (75/260)\n",
      "  Chapter  8  done  (76/260)\n",
      "  Chapter  9  done  (77/260)\n",
      "  Chapter 10  done  (78/260)\n",
      "  Chapter 11  done  (79/260)\n",
      "  Chapter 12  done  (80/260)\n",
      "  Chapter 13  done  (81/260)\n",
      "  Chapter 14  done  (82/260)\n",
      "  Chapter 15  done  (83/260)\n",
      "  Chapter 16  done  (84/260)\n",
      "  Chapter 17  done  (85/260)\n",
      "  Chapter 18  done  (86/260)\n",
      "  Chapter 19  done  (87/260)\n",
      "  Chapter 20  done  (88/260)\n",
      "  Chapter 21  done  (89/260)\n",
      "→ ACT (28 chapters)\n",
      "  Chapter  1  done  (90/260)\n",
      "  Chapter  2  done  (91/260)\n",
      "  Chapter  3  done  (92/260)\n",
      "  Chapter  4  done  (93/260)\n",
      "  Chapter  5  done  (94/260)\n",
      "  Chapter  6  done  (95/260)\n",
      "  Chapter  7  done  (96/260)\n",
      "  Chapter  8  done  (97/260)\n",
      "  Chapter  9  done  (98/260)\n",
      "  Chapter 10  done  (99/260)\n",
      "  Chapter 11  done  (100/260)\n",
      "  Chapter 12  done  (101/260)\n",
      "  Chapter 13  done  (102/260)\n",
      "  Chapter 14  done  (103/260)\n",
      "  Chapter 15  done  (104/260)\n",
      "  Chapter 16  done  (105/260)\n",
      "  Chapter 17  done  (106/260)\n",
      "  Chapter 18  done  (107/260)\n",
      "  Chapter 19  done  (108/260)\n",
      "  Chapter 20  done  (109/260)\n",
      "  Chapter 21  done  (110/260)\n",
      "  Chapter 22  done  (111/260)\n",
      "  Chapter 23  done  (112/260)\n",
      "  Chapter 24  done  (113/260)\n",
      "  Chapter 25  done  (114/260)\n",
      "  Chapter 26  done  (115/260)\n",
      "  Chapter 27  done  (116/260)\n",
      "  Chapter 28  done  (117/260)\n",
      "→ ROM (16 chapters)\n",
      "  Chapter  1  done  (118/260)\n",
      "  Chapter  2  done  (119/260)\n",
      "  Chapter  3  done  (120/260)\n",
      "  Chapter  4  done  (121/260)\n",
      "  Chapter  5  done  (122/260)\n",
      "  Chapter  6  done  (123/260)\n",
      "  Chapter  7  done  (124/260)\n",
      "  Chapter  8  done  (125/260)\n",
      "  Chapter  9  done  (126/260)\n",
      "  Chapter 10  done  (127/260)\n",
      "  Chapter 11  done  (128/260)\n",
      "  Chapter 12  done  (129/260)\n",
      "  Chapter 13  done  (130/260)\n",
      "  Chapter 14  done  (131/260)\n",
      "  Chapter 15  done  (132/260)\n",
      "  Chapter 16  done  (133/260)\n",
      "→ 1CO (16 chapters)\n",
      "  Chapter  1  done  (134/260)\n",
      "  Chapter  2  done  (135/260)\n",
      "  Chapter  3  done  (136/260)\n",
      "  Chapter  4  done  (137/260)\n",
      "  Chapter  5  done  (138/260)\n",
      "  Chapter  6  done  (139/260)\n",
      "  Chapter  7  done  (140/260)\n",
      "  Chapter  8  done  (141/260)\n",
      "  Chapter  9  done  (142/260)\n",
      "  Chapter 10  done  (143/260)\n",
      "  Chapter 11  done  (144/260)\n",
      "  Chapter 12  done  (145/260)\n",
      "  Chapter 13  done  (146/260)\n",
      "  Chapter 14  done  (147/260)\n",
      "  Chapter 15  done  (148/260)\n",
      "  Chapter 16  done  (149/260)\n",
      "→ 2CO (13 chapters)\n",
      "  Chapter  1  done  (150/260)\n",
      "  Chapter  2  done  (151/260)\n",
      "  Chapter  3  done  (152/260)\n",
      "  Chapter  4  done  (153/260)\n",
      "  Chapter  5  done  (154/260)\n",
      "  Chapter  6  done  (155/260)\n",
      "  Chapter  7  done  (156/260)\n",
      "  Chapter  8  done  (157/260)\n",
      "  Chapter  9  done  (158/260)\n",
      "  Chapter 10  done  (159/260)\n",
      "  Chapter 11  done  (160/260)\n",
      "  Chapter 12  done  (161/260)\n",
      "  Chapter 13  done  (162/260)\n",
      "→ GAL (6 chapters)\n",
      "  Chapter  1  done  (163/260)\n",
      "  Chapter  2  done  (164/260)\n",
      "  Chapter  3  done  (165/260)\n",
      "  Chapter  4  done  (166/260)\n",
      "  Chapter  5  done  (167/260)\n",
      "  Chapter  6  done  (168/260)\n",
      "→ EPH (6 chapters)\n",
      "  Chapter  1  done  (169/260)\n",
      "  Chapter  2  done  (170/260)\n",
      "  Chapter  3  done  (171/260)\n",
      "  Chapter  4  done  (172/260)\n",
      "  Chapter  5  done  (173/260)\n",
      "  Chapter  6  done  (174/260)\n",
      "→ PHP (4 chapters)\n",
      "  Chapter  1  done  (175/260)\n",
      "  Chapter  2  done  (176/260)\n",
      "  Chapter  3  done  (177/260)\n",
      "  Chapter  4  done  (178/260)\n",
      "→ COL (4 chapters)\n",
      "  Chapter  1  done  (179/260)\n",
      "  Chapter  2  done  (180/260)\n",
      "  Chapter  3  done  (181/260)\n",
      "  Chapter  4  done  (182/260)\n",
      "→ 1TH (5 chapters)\n",
      "  Chapter  1  done  (183/260)\n",
      "  Chapter  2  done  (184/260)\n",
      "  Chapter  3  done  (185/260)\n",
      "  Chapter  4  done  (186/260)\n",
      "  Chapter  5  done  (187/260)\n",
      "→ 2TH (3 chapters)\n",
      "  Chapter  1  done  (188/260)\n",
      "  Chapter  2  done  (189/260)\n",
      "  Chapter  3  done  (190/260)\n",
      "→ 1TI (6 chapters)\n",
      "  Chapter  1  done  (191/260)\n",
      "  Chapter  2  done  (192/260)\n",
      "  Chapter  3  done  (193/260)\n",
      "  Chapter  4  done  (194/260)\n",
      "  Chapter  5  done  (195/260)\n",
      "  Chapter  6  done  (196/260)\n",
      "→ 2TI (4 chapters)\n",
      "  Chapter  1  done  (197/260)\n",
      "  Chapter  2  done  (198/260)\n",
      "  Chapter  3  done  (199/260)\n",
      "  Chapter  4  done  (200/260)\n",
      "→ TIT (3 chapters)\n",
      "  Chapter  1  done  (201/260)\n",
      "  Chapter  2  done  (202/260)\n",
      "  Chapter  3  done  (203/260)\n",
      "→ PHM (1 chapters)\n",
      "  Chapter  1  done  (204/260)\n",
      "→ HEB (13 chapters)\n",
      "  Chapter  1  done  (205/260)\n",
      "  Chapter  2  done  (206/260)\n",
      "  Chapter  3  done  (207/260)\n",
      "  Chapter  4  done  (208/260)\n",
      "  Chapter  5  done  (209/260)\n",
      "  Chapter  6  done  (210/260)\n",
      "  Chapter  7  done  (211/260)\n",
      "  Chapter  8  done  (212/260)\n",
      "  Chapter  9  done  (213/260)\n",
      "  Chapter 10  done  (214/260)\n",
      "  Chapter 11  done  (215/260)\n",
      "  Chapter 12  done  (216/260)\n",
      "  Chapter 13  done  (217/260)\n",
      "→ JAS (5 chapters)\n",
      "  Chapter  1  done  (218/260)\n",
      "  Chapter  2  done  (219/260)\n",
      "  Chapter  3  done  (220/260)\n",
      "  Chapter  4  done  (221/260)\n",
      "  Chapter  5  done  (222/260)\n",
      "→ 1PE (5 chapters)\n",
      "  Chapter  1  done  (223/260)\n",
      "  Chapter  2  done  (224/260)\n",
      "  Chapter  3  done  (225/260)\n",
      "  Chapter  4  done  (226/260)\n",
      "  Chapter  5  done  (227/260)\n",
      "→ 2PE (3 chapters)\n",
      "  Chapter  1  done  (228/260)\n",
      "  Chapter  2  done  (229/260)\n",
      "  Chapter  3  done  (230/260)\n",
      "→ 1JN (5 chapters)\n",
      "  Chapter  1  done  (231/260)\n",
      "  Chapter  2  done  (232/260)\n",
      "  Chapter  3  done  (233/260)\n",
      "  Chapter  4  done  (234/260)\n",
      "  Chapter  5  done  (235/260)\n",
      "→ 2JN (1 chapters)\n",
      "  Chapter  1  done  (236/260)\n",
      "→ 3JN (1 chapters)\n",
      "  Chapter  1  done  (237/260)\n",
      "→ JUD (1 chapters)\n",
      "  Chapter  1  done  (238/260)\n",
      "→ REV (22 chapters)\n",
      "  Chapter  1  done  (239/260)\n",
      "  Chapter  2  done  (240/260)\n",
      "  Chapter  3  done  (241/260)\n",
      "  Chapter  4  done  (242/260)\n",
      "  Chapter  5  done  (243/260)\n",
      "  Chapter  6  done  (244/260)\n",
      "  Chapter  7  done  (245/260)\n",
      "  Chapter  8  done  (246/260)\n",
      "  Chapter  9  done  (247/260)\n",
      "  Chapter 10  done  (248/260)\n",
      "  Chapter 11  done  (249/260)\n",
      "  Chapter 12  done  (250/260)\n",
      "  Chapter 13  done  (251/260)\n",
      "  Chapter 14  done  (252/260)\n",
      "  Chapter 15  done  (253/260)\n",
      "  Chapter 16  done  (254/260)\n",
      "  Chapter 17  done  (255/260)\n",
      "  Chapter 18  done  (256/260)\n",
      "  Chapter 19  done  (257/260)\n",
      "  Chapter 20  done  (258/260)\n",
      "  Chapter 21  done  (259/260)\n",
      "  Chapter 22  done  (260/260)\n",
      "\n",
      "Done! Saved to: /Users/Dell/Desktop/finalproj/nepal-lang/tamang_bible2.txt\n",
      "Total lines written: 9,319\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "VERSION_ID = 1177\n",
    "OUTPUT_FILE = Path(\"tamang_bible2.txt\")\n",
    "BASE_URL = \"https://www.bible.com/bible/{vid}/{book}.{chap}.TAJNT\"\n",
    "\n",
    "books = [\n",
    "    (\"MAT\", 28), (\"MRK\", 16), (\"LUK\", 24), (\"JHN\", 21),\n",
    "    (\"ACT\", 28), (\"ROM\", 16), (\"1CO\", 16), (\"2CO\", 13),\n",
    "    (\"GAL\", 6),  (\"EPH\", 6),  (\"PHP\", 4),  (\"COL\", 4),\n",
    "    (\"1TH\", 5),  (\"2TH\", 3),  (\"1TI\", 6),  (\"2TI\", 4),\n",
    "    (\"TIT\", 3),  (\"PHM\", 1),  (\"HEB\", 13), (\"JAS\", 5),\n",
    "    (\"1PE\", 5),  (\"2PE\", 3),  (\"1JN\", 5),  (\"2JN\", 1),\n",
    "    (\"3JN\", 1),  (\"JUD\", 1),  (\"REV\", 22)\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove extra spaces, normalize, remove verse numbers from inline text\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'^\\d+\\s*', '', text)  # remove leading verse number if any\n",
    "    return text\n",
    "\n",
    "def scrape_chapter(book_code, chapter_num):\n",
    "    url = BASE_URL.format(vid=VERSION_ID, book=book_code, chap=chapter_num)\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=12, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"  → Failed {book_code} {chapter_num} (status {resp.status_code})\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        content_div = soup.find(\"div\", class_=\"ChapterContent_bible-reader__LmLUa\")\n",
    "        if not content_div:\n",
    "            print(f\"  → No content found for {book_code} {chapter_num}\")\n",
    "            return None\n",
    "\n",
    "        verses = []\n",
    "        for verse_span in content_div.find_all(\"span\", class_=\"ChapterContent_verse__57FIw\"):\n",
    "            label = verse_span.find(\"span\", class_=\"ChapterContent_label__R2PLt\")\n",
    "            verse_num = label.get_text(strip=True) if label else \"?\"\n",
    "            \n",
    "            # Get all text after the label (exclude notes, headings, etc.)\n",
    "            text_parts = []\n",
    "            for child in verse_span.children:\n",
    "                if child.name == \"span\" and \"label\" in child.get(\"class\", []):\n",
    "                    continue\n",
    "                if child.name == \"span\" and \"note\" in child.get(\"class\", []):\n",
    "                    continue  # skip footnotes\n",
    "                text_parts.append(child.get_text(strip=True))\n",
    "            \n",
    "            verse_text = clean_text(\" \".join(text_parts))\n",
    "            if verse_text:\n",
    "                verses.append(f\"{verse_num} {verse_text}\")\n",
    "\n",
    "        return verses\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  → Error on {book_code} {chapter_num}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    all_lines = []\n",
    "    total_chapters = sum(ch for _, ch in books)\n",
    "    processed = 0\n",
    "\n",
    "    print(f\"Starting scrape of Eastern Tamang NT (tajNT) – {len(books)} books, ~{total_chapters} chapters\\n\")\n",
    "\n",
    "    for book_code, max_chap in books:\n",
    "        print(f\"→ {book_code} ({max_chap} chapters)\")\n",
    "        book_lines = [f\"\\n=== {book_code} ===\\n\"]\n",
    "\n",
    "        for chap in range(1, max_chap + 1):\n",
    "            processed += 1\n",
    "            verses = scrape_chapter(book_code, chap)\n",
    "            if verses:\n",
    "                book_lines.append(f\"\\n--- Chapter {chap} ---\\n\")\n",
    "                book_lines.extend(verses)\n",
    "                book_lines.append(\"\")  # blank line\n",
    "            else:\n",
    "                book_lines.append(f\"(Chapter {chap} not available or failed)\")\n",
    "            \n",
    "            print(f\"  Chapter {chap:2d}  done  ({processed}/{total_chapters})\")\n",
    "            time.sleep(1.8)  # polite delay ~0.5–0.6 req/sec\n",
    "\n",
    "        all_lines.extend(book_lines)\n",
    "\n",
    "    # Save to file\n",
    "    with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(all_lines).strip() + \"\\n\")\n",
    "\n",
    "    print(f\"\\nDone! Saved to: {OUTPUT_FILE.absolute()}\")\n",
    "    print(f\"Total lines written: {len(all_lines):,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6625c658-d9ae-49b1-b1fe-492678307b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: tamangnews_scraped1.txt\n",
      "Processing: tamang_bible.txt\n",
      "Processing: tamang_bible2.txt\n",
      "Processing: tamang_scraped2.txt\n",
      "\n",
      "============================================================\n",
      "Files processed           : 4\n",
      "Total lines read          : 9,792\n",
      "Total sentences before dedup : 16,388\n",
      "Duplicate sentences removed  : 1,669\n",
      "Unique sentences kept        : 14,719\n",
      "============================================================\n",
      "\n",
      "Done!\n",
      "Cleaned & deduplicated corpus saved to: tamang_cleaned.txt\n",
      "Final sentence count: 14,719\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import OrderedDict   #to preserve insertion order while removing duplicates\n",
    "\n",
    "# List of input files\n",
    "files_to_merge = [\n",
    "    \"tamangnews_scraped1.txt\",\n",
    "    \"tamang_bible.txt\",\n",
    "    \"tamang_bible2.txt\",\n",
    "    \"tamang_scraped2.txt\"\n",
    "]\n",
    "\n",
    "output_file = \"tamang_cleaned.txt\"\n",
    "\n",
    "# Pattern: remove English letters, ASCII digits, Devanagari digits, and many common punctuation/symbols\n",
    "# You can adjust this pattern if you want to keep more/less characters\n",
    "clean_pattern = re.compile(r\"[A-Za-z0-9\\u0966-\\u096F,.\\-!?;:\\\"'()\\[\\]{}<>@#$%^&*_+=/\\\\|~`]\")\n",
    "\n",
    "# track unique sentences while preserving order\n",
    "seen = OrderedDict()   # key = sentence, value = None (just using it as ordered set)\n",
    "\n",
    "total_lines_read = 0\n",
    "total_sentences_before_dedup = 0\n",
    "total_duplicates_removed = 0\n",
    "\n",
    "for fname in files_to_merge:\n",
    "    if not os.path.exists(fname):\n",
    "        print(f\"File not found, skipping: {fname}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing: {fname}\")\n",
    "    \n",
    "    with open(fname, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            total_lines_read += 1\n",
    "            \n",
    "            # Remove unwanted characters\n",
    "            cleaned = clean_pattern.sub('', line)\n",
    "            \n",
    "            # Normalize multiple spaces → single space\n",
    "            cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "            \n",
    "            if not cleaned:\n",
    "                continue\n",
    "            \n",
    "            # Split into sentences using Devanagari danda (। and ॥)\n",
    "            sentences = re.split(r'[।॥]', cleaned)\n",
    "            \n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if not sent:\n",
    "                    continue\n",
    "                \n",
    "                total_sentences_before_dedup += 1\n",
    "                \n",
    "                #Only keep if we haven't seen it before\n",
    "                if sent not in seen:\n",
    "                    seen[sent] = None\n",
    "                else:\n",
    "                    total_duplicates_removed += 1\n",
    "\n",
    "print(f\"Files processed           : {len(files_to_merge)}\")\n",
    "print(f\"Total lines read          : {total_lines_read:,}\")\n",
    "print(f\"Total sentences before dedup : {total_sentences_before_dedup:,}\")\n",
    "print(f\"Duplicate sentences removed  : {total_duplicates_removed:,}\")\n",
    "print(f\"Unique sentences kept        : {len(seen):,}\")\n",
    "\n",
    "# Write unique sentences in the order they first appeared\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for sentence in seen:\n",
    "        outfile.write(sentence + \"\\n\")\n",
    "\n",
    "print(f\"Done!\")\n",
    "print(f\"Cleaned & deduplicated corpus saved to: {output_file}\")\n",
    "print(f\"Final sentence count: {len(seen):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9b718-fe99-4a67-9d20-a7c2e74bfe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
